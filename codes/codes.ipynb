{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012037c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import log_loss\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import make_scorer, auc, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.svm import l1_min_c\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.gz', compression='gzip', header='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c1d4b",
   "metadata": {},
   "source": [
    "# 1. Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target\n",
    "\n",
    "print(\"Imbalance ratio: {}\".format(float(len(df[df['click']==0]))/len(df[df['click']==1])))\n",
    "print(\"Click-through rate is {}%\".format(100.0*df['click'].sum()/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac455464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see levels for each categorical feature\n",
    "\n",
    "for column in df:\n",
    "    print(f\"{column}: {df[str(column)].unique()}; number of levels: {df[str(column)].nunique()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ee07c",
   "metadata": {},
   "source": [
    "# 2. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d02fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop categories that have only one level (same for all observations)\n",
    "\n",
    "df.drop(['id','app_id', 'app_domain', 'app_category'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3070e8df",
   "metadata": {},
   "source": [
    "# 3. Obtain training and test sets via time split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f83bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.query(\"hour == [16,17,18,19,20,21,22,23] and day_of_week == 1 or hour == [0,1] and day_of_week == 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[:test.index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81116e6",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1da2c",
   "metadata": {},
   "source": [
    "## 4.1. consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9394565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consumer(row):\n",
    "    if str(row['device_id']) != \"a99f214a\":\n",
    "        return row['device_id']\n",
    "    else:\n",
    "        device_ip = row[\"device_ip\"]\n",
    "        device_model = row[\"device_model\"]\n",
    "        return str(device_ip) + str(device_model)\n",
    "    \n",
    "train['consumer'] = train.apply(lambda row: get_user(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237bda2",
   "metadata": {},
   "source": [
    "## 4.2. click_history\n",
    "This tracks the click history made by the same user up to the current point in time, depicted as a string of previous click history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['click'] = df['click'].astype('str')\n",
    "\n",
    "for user in tqdm(df['user'].unique()):\n",
    "    subset = df[df['user'].isin([user])]\n",
    "    click_string = ''.join(subset['click'])\n",
    "    \n",
    "    subset['click_history'] = [click_string[:i-1] for i in range(1, len(click_string)+1)]\n",
    "    \n",
    "    train.loc[subset.index, 'click_history'] = subset['click_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c341bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_empty_string(value):\n",
    "    if value == '':\n",
    "        return 'first string'\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "train['click_history_converted'] = train['click_history'].apply(lambda x: convert_empty_string(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2f3ed",
   "metadata": {},
   "source": [
    "## 4.3. hour_of_day, day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hour'] = train['hour'].map(lambda x: datetime.strptime(str(x),\"%y%m%d%H\"))\n",
    "train['day_of_week'] = train['hour'].map(lambda x: x.weekday()) # 1: tues, 2: wed\n",
    "train['hour'] = train['hour'].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593dabb6",
   "metadata": {},
   "source": [
    "## 4.4. count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"device_ip_count\"] = np.where(train[\"device_ip_count\"] == 1, device_ip_df.loc['Unknown'][0], train[\"device_ip_count\"])\n",
    "train[\"device_id_count\"] = np.where(train[\"device_id_count\"] == 1, device_id_df.loc['Unknown'][0], train[\"device_id_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_ip_count(device_ip):\n",
    "    return device_ip_df.loc[device_ip][0]\n",
    "    \n",
    "train['device_ip_count'] = train['device_ip'].apply(lambda x: get_device_ip_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fda117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_id_count(device_id):\n",
    "    return device_id_df.loc[device_id][0]\n",
    "    \n",
    "train['device_id_count'] = train['device_id'].apply(lambda x: get_device_id_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b22d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hour_count(hour):\n",
    "    return hourly_impression_df.loc[hour][0]\n",
    "    \n",
    "train['hour_count'] = train['hour'].apply(lambda x: get_hour_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04346dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_consumer_df = pd.DataFrame(train.groupby([\"hour\", \"consumer\"]).size().unstack())\n",
    "\n",
    "def get_hourly_consumer_count(row):\n",
    "    hour = row['hour']\n",
    "    consumer = row['consumer']\n",
    "    \n",
    "    desired = hourly_consumer_df.loc[hour, consumer]\n",
    "    \n",
    "    if desired == np.nan:\n",
    "        return 0\n",
    "    else:\n",
    "        return desired\n",
    "    \n",
    "train['hourly_consumer_count'] = train.apply(lambda row: get_hourly_consumer_count(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455dcdc",
   "metadata": {},
   "source": [
    "# 5. Feature cleaning of rare features\n",
    "For catgorical features, we remove rare feature values, defined as levels of the feature that only appear once. We group such levels together into a \"Rare\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a34994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rare_features(col):\n",
    "    col_counts = pd.DataFrame(train.groupby(col).size())\n",
    "    col_counts[0] = np.where(col_counts[0] == 1, \"Rare\", col_counts[0])\n",
    "    rare_rows = col_counts[col_counts[0] == \"Rare\"].index\n",
    "    train[col] = np.where(train[col].isin(rare_rows), \"Rare\", train[col])\n",
    "\n",
    "for col in ['site_id', 'site_domain', 'device_id', 'device_ip', 'device_model']:\n",
    "    clean_rare_features(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9555b8",
   "metadata": {},
   "source": [
    "# 6. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ec5df",
   "metadata": {},
   "source": [
    "## 6.1. Hash Encoding: For features with high cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab09174",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.HashingEncoder()\n",
    "\n",
    "def hash_encode(self):\n",
    "    object_list_columns = self.columns\n",
    "    object_list_dtypes = self.dtypes\n",
    "    new_col_suffix = '_int'\n",
    "    for index in range(0,len(object_list_columns)):\n",
    "        if object_list_dtypes[index] == object :\n",
    "            self[object_list_columns[index]+new_col_suffix] = self[object_list_columns[index]].map(lambda x: encoder.fit_transform(x))\n",
    "            self.drop([object_list_columns[index]],inplace=True,axis=1)\n",
    "    return self\n",
    "\n",
    "train = hash_encode(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9c4d7",
   "metadata": {},
   "source": [
    "# 7. Train & tune CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a86df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop([\"click\"],axis=1)\n",
    "y_train = train[\"click\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23bb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_f = ['hour', 'C1', 'banner_pos', 'site_id', 'site_domain','site_category','device_id','device_ip','device_model',\n",
    "                 'device_type','device_conn_type', 'C14','C15','C16','C17','C18', 'C19','C20','C21',\n",
    "                 'day_of_week', 'user', 'click_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007527c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = CatBoostClassifier(loss_function='Logloss', cat_features=categorical_f, data_partition='FeatureParallel', \n",
    "                              bootstrap_type='Bernoulli', verbose=10)\n",
    "\n",
    "param = {\n",
    "    'iterations':Integer(100, 250), # on the low side to speed up computation (learning rate will adjust accordingly)\n",
    "    'depth':Integer(1, 10),\n",
    "    'random_strength':Real(1e-9, 10), # amount of randomness to use for scoring splits (used to prevent overfitting)\n",
    "    #'bagging_temperature':Real(0.0, 1.0),\n",
    "    'l2_leaf_reg':Real(0.001, 10000), # coefficient at the L2 regularization term (lambda)\n",
    "    'scale_pos_weight':Real(1, 50), # weight for class 1 in binary classification\n",
    "    'subsample':Real(0.5, 1),\n",
    "    'colsample_bylevel':Real(0.5,1),\n",
    "    'model_size_reg':Real(0.01, 1000), # model size regularization coefficient\n",
    "    'leaf_estimation_iterations':[1,5] # how many steps are done in every tree when calculating leaf values (values recommendated in documentation)\n",
    "}\n",
    "\n",
    "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True) \n",
    "    \n",
    "opt = BayesSearchCV(catboost, param, scoring = LogLoss, n_iter=16, cv=3, random_state=_, verbose=1)\n",
    "\n",
    "# executes bayesian optimization\n",
    "opt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67587c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_pred = opt.predict_proba(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e9585",
   "metadata": {},
   "source": [
    "# 8. Train & tune LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', metric='binary_logloss')\n",
    "\n",
    "param = {\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'learning_rate': Real(0.01, 0.3),\n",
    "    'feature_fraction': Real(0.2, 0.9, 'uniform'),\n",
    "    'bagging_fraction': Real(0.2, 0.9),\n",
    "    'max_bin': Integer(20, 255, 'uniform'),\n",
    "    'n_estimators': Integer(100, 1000, 'uniform'),\n",
    "    'num_leaves': Integer(24, 80, 'uniform'),\n",
    "    'min_sum_hessian_in_leaf':Integer(0,100, 'uniform'),\n",
    "    'min_data_in_leaf': Integer(20, 100, 'uniform'),\n",
    "    'min_split_gain': Real(0.001, 0.1),\n",
    "    'lambda_l1': Real(1e-8, 10.0),\n",
    "    'lambda_l2': Real(1e-8, 10.0),\n",
    "    'bagging_freq': Integer(1,7, 'uniform')\n",
    "}\n",
    "\n",
    "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# input your random_state\n",
    "opt = BayesSearchCV(\n",
    "    lgb_model,\n",
    "    param,\n",
    "    scoring = LogLoss,\n",
    "    n_iter=32,\n",
    "    cv=5,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_pred = opt.predict_proba(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1abc6c",
   "metadata": {},
   "source": [
    "# 8. Ensemble models by Stacking, with Elastic Net Logistic Regression as meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "cs = l1_min_c(prob_df, y_test, loss='log') * np.logspace(0, 10, 20)\n",
    "clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "param_grid = {'C': list(cs)}\n",
    "grid = GridSearchCV(clf, param_grid, verbose=10,cv=3, scoring=LogLoss)\n",
    "grid.fit(prob_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_opt = LogisticRegression(C=0.7288911003040416, penalty='l1', solver='liblinear')\n",
    "clf_opt.fit(prob_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = clf_opt.coef_[0] / clf_opt.coef_[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_proba = np.zeros((len(prob_df), ))\n",
    "final_proba+=proba_lgb_li[0] * wts[0]\n",
    "final_proba+=proba_lgb_li_8[0] * wts[1]\n",
    "final_proba+=proba_lgb_li_28[0] * wts[2]\n",
    "final_proba+=proba_lgb_li_4812[0] * wts[3]\n",
    "final_proba+=proba_lgb_li_19[0] * wts[4]\n",
    "final_proba+=proba_lgb_li_48128 * wts[5]\n",
    "final_proba+=proba_xgb_li[0] * wts[6]\n",
    "final_proba+=proba_cat_li_2[0] * wts[7]\n",
    "final_proba+=proba_cat_li_240[0] * wts[8]\n",
    "final_proba+=proba_cat_li_101[0] * wts[9]\n",
    "final_proba+=proba_cat_li_16[0] * wts[10]\n",
    "final_proba+=proba_cat_li_24[0] * wts[11]\n",
    "log_loss(y_test, final_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
